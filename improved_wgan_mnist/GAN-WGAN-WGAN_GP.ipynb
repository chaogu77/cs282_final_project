{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference\n",
    "### Loss Function \n",
    "### GAN\n",
    "- $D$: $max_D V(D) = E_{x\\sim p_{data}}(x)[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]$\n",
    "- $G$: $min_G V(G) = E_{z\\sim p_z(z)}[log(1-D(G(z))]$\n",
    "\n",
    "### WGAN\n",
    "- $D$: $max_D V(D) = E_{x\\sim p_{data}}[D(x)] - E_{z\\sim p_z(z)}[D(G(z)]$\n",
    "- $G$: $max_G V(G) = E_{z\\sim p_z(z)}[D(G(z)]$\n",
    "                                        \n",
    "### WGAN-GP\n",
    "- $D$: $max_D V(D) = E_{x\\sim p_{data}}[D(x)] - E_{z\\sim p_z(z)}[D(G(z)] + \\lambda(\\lVert \\nabla D(\\hat{x}) \\rVert_{2}-1)^2$\n",
    "- $G$: $max_G V(G) = E_{z\\sim p_z(z)}[D(G(z)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Features                      |  GAN   | WGAN |WGAN-GP\n",
    "| --------------------------- |:------:|:----: |:-----\n",
    "|output layer of Discriminator |Sigmoid | Linear |Linear\n",
    "|optimizer                     | Adam   | RMS  | Adam\n",
    "|weight clipping               | False  | True | False\n",
    "|Batch Normalization           | False  | True | False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-selected the hyper parameters for you this time. You usually need to tune this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "num_steps = 100000 #num of iteration\n",
    "batch_size = 50\n",
    "learning_rate = 0.0002\n",
    "Iters = 5\n",
    "# c = 0.01\n",
    "_lambda = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden dimensions of the generator and the discriminator are also prespecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 128 # Noise data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(noises, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        # hidden layer with name \"g_hidden\"\n",
    "        hidden = tf.layers.dense(noises, gen_hidden_dim, tf.nn.relu, name='g_hidden')\n",
    "        # out layer with name \"g_out\"\n",
    "        out_images = tf.layers.dense(hidden, image_dim, tf.nn.sigmoid, name='g_out')\n",
    "    return out_images\n",
    "\n",
    "# Discriminator\n",
    "def discriminator(images, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()            \n",
    "        # hidden layer with name \"d_hidden\"\n",
    "        hidden = tf.layers.dense(images, disc_hidden_dim, tf.nn.relu, name='d_hidden')\n",
    "        # out layer with name \"d_out\"\n",
    "        out_prob = tf.layers.dense(hidden, 1, None, name='d_out')\n",
    "    return out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the inputs to generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input noise to G and generate images.\n",
    "This should be a one linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sample = generator(gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input real and fake images to D and get predictions.\n",
    "For D, you should have two inputs: real data and fake data. The latter is the output of $G$. For the latter, set `reuse=True`. I won't go into detail about it, but basically, you are reusing the samve variables in the above `discriminator` function and so you want to make them reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss = - disc_fake\n",
    "disc_loss = -disc_real +  disc_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip = [p.assign(tf.clip_by_value(p,-c,c))for p in disc_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = tf.random_uniform(shape=[Batch_Size,1],minval=0.,maxval=1.)\n",
    "differences = fake_data-real_data\n",
    "interpolates = real_data + (alpha*differences)\n",
    "gradients = tf.gradients(discriminator(interpolates, reuse=True),[interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients),reduction_indices=[1]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "disc_cost += _Lambda*gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Minimize (or maximize) the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "disc_vars = [var for var in tvars if 'd_' in var.name]\n",
    "gen_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the model.\n",
    "For each iteration, take some batch of MNIST. Generate a prior noise $z$ by `np.random.uniform(-1., 1., size=[batch_size, noise_dim])`. Feed the batch data and prior noise to the model to update the objective.\n",
    "\n",
    "After some epochs of training, for each noise generated, get the output $x$ by the generator and plot it using matplotlib. This time we prepared the code for you but read through it to understand it. Then, change the variable names if they are different from yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "        # Train\n",
    "        feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "        _,gl = sess.run([train_gen,  gen_loss, ],\n",
    "                                feed_dict=feed_dict)\n",
    "        \n",
    "        for i in range(Iters):\n",
    "            -,dl = = sess.run([train_disc,  disc_loss, ],\n",
    "                                    feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 1000 == 0 or step == 1:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (step, gl, dl))\n",
    "    \n",
    "        # Generate images from noise, using the generator network.\n",
    "        if step % 10000 == 0 or step == 1:\n",
    "            f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "            for i in range(10):\n",
    "                # Noise input.\n",
    "                z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "                g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "                g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "                # Reverse colours for better display\n",
    "                g = -1 * (g - 1)\n",
    "                for j in range(4):\n",
    "                    # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "                    img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "                                     newshape=(28, 28, 3))\n",
    "                    a[j][i].imshow(img)\n",
    "\n",
    "            plt.draw()\n",
    "            print('gan'+str(step)+'.png')\n",
    "            plt.savefig('gan'+str(step)+'.png')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
