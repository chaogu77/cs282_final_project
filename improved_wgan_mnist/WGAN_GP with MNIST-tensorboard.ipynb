{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 128 # Noise data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN_GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning param\n",
    "Batch_Size = 50\n",
    "Critic_Iters = 5 # for WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "Lambda = 10 # gradient penalty lambda hyperparameter\n",
    "Iters = 100001 # number of generator iterations to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('g_h'):\n",
    "    g_W1 = tf.Variable(tf.random_normal([noise_dim,gen_hidden_dim]),name='g_W1')\n",
    "    g_b1 = tf.Variable(tf.random_normal([gen_hidden_dim]),name='g_b1')\n",
    "with tf.name_scope('g_o'):\n",
    "    g_W2 = tf.Variable(tf.random_normal([gen_hidden_dim,image_dim]),name='g_W2')\n",
    "    g_b2 = tf.Variable(tf.random_normal([image_dim]),name='g_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('d_h'):\n",
    "    d_W1 = tf.Variable(tf.random_normal([image_dim,disc_hidden_dim]),name='d_W1')\n",
    "    d_b1 = tf.Variable(tf.random_normal([disc_hidden_dim]),name='d_b1')\n",
    "with tf.name_scope('g_o'):\n",
    "    d_W2 = tf.Variable(tf.random_normal([disc_hidden_dim,1]),name='d_W2')\n",
    "    d_b2 = tf.Variable(tf.random_normal([1]),name='d_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "with tf.name_scope('Generator'):\n",
    "    def generator(noises, reuse=False):\n",
    "        with tf.variable_scope('generator') as scope:\n",
    "            if (reuse):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            # hidden layer with name \"g_hidden\"\n",
    "            hidden = tf.nn.relu(noises @ g_W1 + g_b1, name='gen_hidden')\n",
    "            # out layer with name \"g_out\"\n",
    "            out_images = tf.nn.sigmoid(hidden @ g_W2 + g_b2, name='gen_out')\n",
    "        return out_images\n",
    "\n",
    "# Discriminator\n",
    "with tf.name_scope('Discriminator'):\n",
    "    def discriminator(images, reuse=False):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if (reuse):\n",
    "                tf.get_variable_scope().reuse_variables()            \n",
    "            # hidden layer with name \"d_hidden\"\n",
    "            hidden = tf.nn.relu(images @ d_W1 + d_b1, name='disc_hidden')\n",
    "            # out layer with name \"d_out\"\n",
    "            out = tf.add(hidden @ d_W2, d_b2,name = 'disc_out')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = generator(gen_input)\n",
    "real_data = tf.placeholder(tf.float32, shape=[None, image_dim], name='real_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = discriminator(real_data)\n",
    "disc_fake = discriminator(fake_data, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "disc_vars = [var for var in tvars if 'd_' in var.name]\n",
    "gen_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = tf.random_uniform(shape=[Batch_Size,1],minval=0.,maxval=1.)\n",
    "differences = fake_data-real_data\n",
    "interpolates = real_data + (alpha*differences)\n",
    "gradients = tf.gradients(discriminator(interpolates, reuse=True),[interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients),reduction_indices=[1]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "disc_cost += Lambda*gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = tf.train.AdamOptimizer(\n",
    "        learning_rate=1e-4, \n",
    "        beta1=0.5,\n",
    "        beta2=0.9\n",
    "    ).minimize(gen_cost, var_list=gen_vars)\n",
    "\n",
    "train_disc = tf.train.AdamOptimizer(\n",
    "        learning_rate=1e-4, \n",
    "        beta1=0.5, \n",
    "        beta2=0.9\n",
    "    ).minimize(disc_cost, var_list=disc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    loss = tf.summary.scalar('loss',-disc_cost)\n",
    "    \n",
    "    gen_W1 = tf.summary.scalar('g_W1',tf.reduce_mean(g_W1))\n",
    "    gen_b1 = tf.summary.scalar('g_b1',tf.reduce_mean(g_b1))\n",
    "    gen_W2 = tf.summary.scalar('g_W2',tf.reduce_mean(g_W2))\n",
    "    gen_b2 = tf.summary.scalar('g_b2',tf.reduce_mean(g_b2))\n",
    "    \n",
    "    disc_W1 = tf.summary.scalar('d_W1',tf.reduce_mean(d_W1))\n",
    "    disc_b1 = tf.summary.scalar('d_b1',tf.reduce_mean(d_b1))\n",
    "    disc_W2 = tf.summary.scalar('d_W2',tf.reduce_mean(d_W2))\n",
    "    disc_bb = tf.summary.scalar('d_b2',tf.reduce_mean(d_b2))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Generator Loss: 114.657654, Discriminator Loss: 1105188.625000\n",
      "wgan_gp0.png\n",
      "Step 1000: Generator Loss: -35.852932, Discriminator Loss: 111672.882812\n",
      "Step 2000: Generator Loss: -28.763277, Discriminator Loss: 22722.531250\n",
      "Step 3000: Generator Loss: -5.601259, Discriminator Loss: 2372.725342\n",
      "Step 4000: Generator Loss: 2.792879, Discriminator Loss: 230.540619\n",
      "Step 5000: Generator Loss: 10.454459, Discriminator Loss: 13.533514\n",
      "Step 6000: Generator Loss: 6.770102, Discriminator Loss: -5.961549\n",
      "Step 7000: Generator Loss: 1.877961, Discriminator Loss: 2.025961\n",
      "Step 8000: Generator Loss: -1.159017, Discriminator Loss: 12.308200\n",
      "Step 9000: Generator Loss: -1.267867, Discriminator Loss: 3.602905\n",
      "Step 10000: Generator Loss: -1.310349, Discriminator Loss: 12.007820\n",
      "wgan_gp10000.png\n",
      "Step 11000: Generator Loss: -0.566052, Discriminator Loss: -4.430213\n",
      "Step 12000: Generator Loss: -0.634516, Discriminator Loss: -4.527105\n",
      "Step 13000: Generator Loss: -0.878309, Discriminator Loss: -4.867041\n",
      "Step 14000: Generator Loss: -1.171261, Discriminator Loss: -5.001824\n",
      "Step 15000: Generator Loss: -1.496945, Discriminator Loss: -4.922032\n",
      "Step 16000: Generator Loss: -1.859507, Discriminator Loss: -5.240273\n",
      "Step 17000: Generator Loss: -2.018563, Discriminator Loss: -5.136477\n",
      "Step 18000: Generator Loss: -2.495998, Discriminator Loss: -5.455905\n",
      "Step 19000: Generator Loss: -2.610732, Discriminator Loss: -5.162374\n",
      "Step 20000: Generator Loss: -2.857636, Discriminator Loss: -5.273406\n",
      "wgan_gp20000.png\n",
      "Step 21000: Generator Loss: -2.927187, Discriminator Loss: -5.212907\n",
      "Step 22000: Generator Loss: -3.647387, Discriminator Loss: -5.224473\n",
      "Step 23000: Generator Loss: -3.930727, Discriminator Loss: -5.231248\n",
      "Step 24000: Generator Loss: -4.306032, Discriminator Loss: -4.997058\n",
      "Step 25000: Generator Loss: -4.585327, Discriminator Loss: -5.127585\n",
      "Step 26000: Generator Loss: -4.539793, Discriminator Loss: -4.788754\n",
      "Step 27000: Generator Loss: -4.602545, Discriminator Loss: -4.712016\n",
      "Step 28000: Generator Loss: -5.163327, Discriminator Loss: -4.799459\n",
      "Step 29000: Generator Loss: -5.646657, Discriminator Loss: -4.931712\n",
      "Step 30000: Generator Loss: -5.705798, Discriminator Loss: -4.764153\n",
      "wgan_gp30000.png\n",
      "Step 31000: Generator Loss: -6.312645, Discriminator Loss: -4.587414\n",
      "Step 32000: Generator Loss: -6.442152, Discriminator Loss: -4.308909\n",
      "Step 33000: Generator Loss: -6.435131, Discriminator Loss: -4.335203\n",
      "Step 34000: Generator Loss: -6.318672, Discriminator Loss: -4.452555\n",
      "Step 35000: Generator Loss: -6.904856, Discriminator Loss: -4.242661\n",
      "Step 36000: Generator Loss: -7.024161, Discriminator Loss: -4.253726\n",
      "Step 37000: Generator Loss: -7.478352, Discriminator Loss: -4.220047\n",
      "Step 38000: Generator Loss: -7.645595, Discriminator Loss: -4.523216\n",
      "Step 39000: Generator Loss: -7.742726, Discriminator Loss: -4.022775\n",
      "Step 40000: Generator Loss: -7.905999, Discriminator Loss: -3.968945\n",
      "wgan_gp40000.png\n",
      "Step 41000: Generator Loss: -8.219757, Discriminator Loss: -4.216517\n",
      "Step 42000: Generator Loss: -7.865605, Discriminator Loss: -3.998785\n",
      "Step 43000: Generator Loss: -7.889889, Discriminator Loss: -3.691637\n",
      "Step 44000: Generator Loss: -8.156888, Discriminator Loss: -3.864964\n",
      "Step 45000: Generator Loss: -7.713218, Discriminator Loss: -3.685600\n",
      "Step 46000: Generator Loss: -8.139333, Discriminator Loss: -3.605495\n",
      "Step 47000: Generator Loss: -8.483295, Discriminator Loss: -3.903113\n",
      "Step 48000: Generator Loss: -9.074984, Discriminator Loss: -3.367846\n",
      "Step 49000: Generator Loss: -8.912964, Discriminator Loss: -3.756919\n",
      "Step 50000: Generator Loss: -9.493095, Discriminator Loss: -3.455182\n",
      "wgan_gp50000.png\n",
      "Step 51000: Generator Loss: -9.729944, Discriminator Loss: -3.442804\n",
      "Step 52000: Generator Loss: -9.965209, Discriminator Loss: -3.337558\n",
      "Step 53000: Generator Loss: -10.055994, Discriminator Loss: -3.575353\n",
      "Step 54000: Generator Loss: -10.590659, Discriminator Loss: -3.340603\n",
      "Step 55000: Generator Loss: -10.653137, Discriminator Loss: -3.164687\n",
      "Step 56000: Generator Loss: -10.745811, Discriminator Loss: -3.076890\n",
      "Step 57000: Generator Loss: -9.941331, Discriminator Loss: -3.182023\n",
      "Step 58000: Generator Loss: -10.092776, Discriminator Loss: -2.978127\n",
      "Step 59000: Generator Loss: -10.289111, Discriminator Loss: -3.125776\n",
      "Step 60000: Generator Loss: -10.203167, Discriminator Loss: -3.341719\n",
      "wgan_gp60000.png\n",
      "Step 61000: Generator Loss: -10.731206, Discriminator Loss: -3.105794\n",
      "Step 62000: Generator Loss: -10.808246, Discriminator Loss: -2.976138\n",
      "Step 63000: Generator Loss: -11.346689, Discriminator Loss: -2.843453\n",
      "Step 64000: Generator Loss: -11.272098, Discriminator Loss: -2.898490\n",
      "Step 65000: Generator Loss: -11.477623, Discriminator Loss: -2.716397\n",
      "Step 66000: Generator Loss: -11.817589, Discriminator Loss: -2.985025\n",
      "Step 67000: Generator Loss: -12.272195, Discriminator Loss: -2.641924\n",
      "Step 68000: Generator Loss: -12.513201, Discriminator Loss: -2.513988\n",
      "Step 69000: Generator Loss: -12.522081, Discriminator Loss: -2.672202\n",
      "Step 70000: Generator Loss: -12.701534, Discriminator Loss: -2.814293\n",
      "wgan_gp70000.png\n",
      "Step 71000: Generator Loss: -13.074947, Discriminator Loss: -2.529612\n",
      "Step 72000: Generator Loss: -13.470738, Discriminator Loss: -2.573111\n",
      "Step 73000: Generator Loss: -13.864838, Discriminator Loss: -2.464202\n",
      "Step 74000: Generator Loss: -13.842329, Discriminator Loss: -2.429990\n",
      "Step 75000: Generator Loss: -14.547211, Discriminator Loss: -2.678489\n",
      "Step 76000: Generator Loss: -14.911681, Discriminator Loss: -2.374403\n",
      "Step 77000: Generator Loss: -15.247047, Discriminator Loss: -2.694730\n",
      "Step 78000: Generator Loss: -15.901142, Discriminator Loss: -2.491898\n",
      "Step 79000: Generator Loss: -16.298531, Discriminator Loss: -2.336578\n",
      "Step 80000: Generator Loss: -16.344460, Discriminator Loss: -2.680322\n",
      "wgan_gp80000.png\n",
      "Step 81000: Generator Loss: -16.505407, Discriminator Loss: -2.286573\n",
      "Step 82000: Generator Loss: -16.792717, Discriminator Loss: -2.575438\n",
      "Step 83000: Generator Loss: -17.273491, Discriminator Loss: -2.325740\n",
      "Step 84000: Generator Loss: -17.777912, Discriminator Loss: -2.181311\n",
      "Step 85000: Generator Loss: -18.251728, Discriminator Loss: -2.365315\n",
      "Step 86000: Generator Loss: -18.556259, Discriminator Loss: -2.362624\n",
      "Step 87000: Generator Loss: -19.482077, Discriminator Loss: -2.416159\n",
      "Step 88000: Generator Loss: -19.909510, Discriminator Loss: -2.281115\n",
      "Step 89000: Generator Loss: -20.595242, Discriminator Loss: -2.256331\n",
      "Step 90000: Generator Loss: -21.234751, Discriminator Loss: -2.278411\n",
      "wgan_gp90000.png\n",
      "Step 91000: Generator Loss: -21.422186, Discriminator Loss: -2.354945\n",
      "Step 92000: Generator Loss: -22.322884, Discriminator Loss: -2.384955\n",
      "Step 93000: Generator Loss: -22.833338, Discriminator Loss: -2.190089\n",
      "Step 94000: Generator Loss: -23.742701, Discriminator Loss: -2.111076\n",
      "Step 95000: Generator Loss: -23.891016, Discriminator Loss: -2.175592\n",
      "Step 96000: Generator Loss: -24.583599, Discriminator Loss: -2.000118\n",
      "Step 97000: Generator Loss: -24.904488, Discriminator Loss: -2.141657\n",
      "Step 98000: Generator Loss: -25.627928, Discriminator Loss: -2.101705\n",
      "Step 99000: Generator Loss: -26.098433, Discriminator Loss: -2.056727\n",
      "Step 100000: Generator Loss: -26.775959, Discriminator Loss: -2.346190\n",
      "wgan_gp100000.png\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('WGAN_GP_log',sess.graph,flush_secs = 30)\n",
    "    for step in range(Iters):\n",
    "\n",
    "        batch_x, _ = mnist.train.next_batch(Batch_Size)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[Batch_Size, noise_dim])\n",
    "        \n",
    "        # train discriminator\n",
    "        for i in range(Critic_Iters):\n",
    "            _,dl = sess.run([train_disc,disc_cost],\n",
    "                                   feed_dict={real_data:batch_x,gen_input:z})\n",
    "        \n",
    "        # train generator\n",
    "        _,gl=sess.run([train_gen,gen_cost],\n",
    "                      feed_dict={gen_input:z})\n",
    "        \n",
    "        # keep log\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (step, gl, dl))\n",
    "        if step % 100 == 0:\n",
    "            summary = sess.run(summary_op,feed_dict={gen_input:z, real_data:batch_x})\n",
    "            writer.add_summary(summary,global_step = step)\n",
    "            \n",
    "        # Generate images from noise, using the generator network.\n",
    "        if step % 10000 == 0:\n",
    "            f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "            for i in range(10):\n",
    "                # Noise input.\n",
    "                z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "                g = sess.run([fake_data], feed_dict={gen_input: z})\n",
    "                g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "                # Reverse colours for better display\n",
    "                g = -1 * (g - 1)\n",
    "                \n",
    "                for j in range(4):\n",
    "                    # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "                    img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "                                     newshape=(28, 28, 3))\n",
    "                    a[j][i].imshow(img)\n",
    "\n",
    "            plt.draw()\n",
    "            print('wgan_gp'+str(step)+'.png')\n",
    "            plt.savefig('wgan_gp'+str(step)+'.png')\n",
    "    print('Done!')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
