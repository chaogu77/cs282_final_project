{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. GAN\n",
    "This section will be an exercise. Surprisingly, you can build GAN fairly easily just by using the concepts we learned so far.\n",
    "\n",
    "## Preparation\n",
    "Read section 1, 2, 3 of the original [GAN paper](https://arxiv.org/pdf/1406.2661.pdf). Then, follow the next 7 steps to implement GAN.\n",
    "\n",
    "To summarie, we have the following problem setup:\n",
    "- $x$: data with distribution $p_{data}$\n",
    "- $p_g$: distribution trained by the generator\n",
    "- $z$: prior input noise variables\n",
    "- $p_z$: prior of $z$\n",
    "- $G(z;\\theta_G)$: generator neural network with parameter $\\theta_G$\n",
    "- $D(z;\\theta_D)$: discriminator neural network with parameter $\\theta_D$\n",
    "\n",
    "The goal for $D,G$ are the following:\n",
    "- $D$: $max_D V(D) = E_{x\\sim p_{data}}(x)[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]$\n",
    "- $G$: $min_G V(G) = E_{z\\sim p_z(z)}[log(1-D(G(z))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-selected the hyper parameters for you this time. You usually need to tune this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "num_steps = 500000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden dimensions of the generator and the discriminator are also prespecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100 # Noise data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(noises, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        # hidden layer with name \"g_hidden\"\n",
    "        hidden = tf.layers.dense(noises, gen_hidden_dim, tf.nn.relu, name='g_hidden')\n",
    "        # out layer with name \"g_out\"\n",
    "        out_images = tf.layers.dense(hidden, image_dim, tf.nn.sigmoid, name='g_out')\n",
    "    return out_images\n",
    "\n",
    "# Discriminator\n",
    "def discriminator(images, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()            \n",
    "        # hidden layer with name \"d_hidden\"\n",
    "        hidden = tf.layers.dense(images, disc_hidden_dim, tf.nn.relu, name='d_hidden')\n",
    "        # out layer with name \"d_out\"\n",
    "        out_prob = tf.layers.dense(hidden, 1, tf.nn.sigmoid, name='d_out')\n",
    "    return out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the inputs to generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input noise to G and generate images.\n",
    "This should be a one linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sample = generator(gen_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input real and fake images to D and get predictions.\n",
    "For D, you should have two inputs: real data and fake data. The latter is the output of $G$. For the latter, set `reuse=True`. I won't go into detail about it, but basically, you are reusing the samve variables in the above `discriminator` function and so you want to make them reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define the objective.\n",
    "Expectation should be approximated using the sample mean. As a reminder, they are:\n",
    "- $D$: $max_D V(D) = E_{x\\sim p_{data}}(x)[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]$\n",
    "- $G$: $min_G V(G) = E_{z\\sim p_z(z)}[log(1-D(G(z))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1. - disc_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Minimize (or maximize) the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "disc_vars = [var for var in tvars if 'd_' in var.name]\n",
    "gen_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the model.\n",
    "For each iteration, take some batch of MNIST. Generate a prior noise $z$ by `np.random.uniform(-1., 1., size=[batch_size, noise_dim])`. Feed the batch data and prior noise to the model to update the objective.\n",
    "\n",
    "After some epochs of training, for each noise generated, get the output $x$ by the generator and plot it using matplotlib. This time we prepared the code for you but read through it to understand it. Then, change the variable names if they are different from yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generator Loss: 0.493318, Discriminator Loss: 1.622798\n",
      "gan1.png\n",
      "Step 1000: Generator Loss: 3.906511, Discriminator Loss: 0.054000\n",
      "Step 2000: Generator Loss: 4.335425, Discriminator Loss: 0.036723\n",
      "Step 3000: Generator Loss: 5.399007, Discriminator Loss: 0.024428\n",
      "Step 4000: Generator Loss: 4.334804, Discriminator Loss: 0.048920\n",
      "Step 5000: Generator Loss: 4.469631, Discriminator Loss: 0.031950\n",
      "Step 6000: Generator Loss: 4.577167, Discriminator Loss: 0.040134\n",
      "Step 7000: Generator Loss: 4.796434, Discriminator Loss: 0.100180\n",
      "Step 8000: Generator Loss: 4.574548, Discriminator Loss: 0.052114\n",
      "Step 9000: Generator Loss: 5.035746, Discriminator Loss: 0.068192\n",
      "Step 10000: Generator Loss: 4.918055, Discriminator Loss: 0.060593\n",
      "gan10000.png\n",
      "Step 11000: Generator Loss: 5.496971, Discriminator Loss: 0.026692\n",
      "Step 12000: Generator Loss: 5.443554, Discriminator Loss: 0.083956\n",
      "Step 13000: Generator Loss: 5.665005, Discriminator Loss: 0.047589\n",
      "Step 14000: Generator Loss: 5.246143, Discriminator Loss: 0.022216\n",
      "Step 15000: Generator Loss: 5.529854, Discriminator Loss: 0.068529\n",
      "Step 16000: Generator Loss: 6.355235, Discriminator Loss: 0.032296\n",
      "Step 17000: Generator Loss: 6.252532, Discriminator Loss: 0.028969\n",
      "Step 18000: Generator Loss: 6.041484, Discriminator Loss: 0.079785\n",
      "Step 19000: Generator Loss: 5.110085, Discriminator Loss: 0.168794\n",
      "Step 20000: Generator Loss: 6.081274, Discriminator Loss: 0.077383\n",
      "gan20000.png\n",
      "Step 21000: Generator Loss: 5.984705, Discriminator Loss: 0.157702\n",
      "Step 22000: Generator Loss: 5.164141, Discriminator Loss: 0.071095\n",
      "Step 23000: Generator Loss: 5.167514, Discriminator Loss: 0.098412\n",
      "Step 24000: Generator Loss: 4.968740, Discriminator Loss: 0.144271\n",
      "Step 25000: Generator Loss: 5.550411, Discriminator Loss: 0.083578\n",
      "Step 26000: Generator Loss: 5.997223, Discriminator Loss: 0.086491\n",
      "Step 27000: Generator Loss: 5.282353, Discriminator Loss: 0.134262\n",
      "Step 28000: Generator Loss: 5.805591, Discriminator Loss: 0.122093\n",
      "Step 29000: Generator Loss: 4.651209, Discriminator Loss: 0.294990\n",
      "Step 30000: Generator Loss: 5.059191, Discriminator Loss: 0.103777\n",
      "gan30000.png\n",
      "Step 31000: Generator Loss: 4.225809, Discriminator Loss: 0.184616\n",
      "Step 32000: Generator Loss: 4.194739, Discriminator Loss: 0.167177\n",
      "Step 33000: Generator Loss: 5.603895, Discriminator Loss: 0.078377\n",
      "Step 34000: Generator Loss: 5.884189, Discriminator Loss: 0.053834\n",
      "Step 35000: Generator Loss: 4.455955, Discriminator Loss: 0.166711\n",
      "Step 36000: Generator Loss: 4.068246, Discriminator Loss: 0.279758\n",
      "Step 37000: Generator Loss: 4.813500, Discriminator Loss: 0.126670\n",
      "Step 38000: Generator Loss: 6.541212, Discriminator Loss: 0.076800\n",
      "Step 39000: Generator Loss: 4.576997, Discriminator Loss: 0.150012\n",
      "Step 40000: Generator Loss: 5.600259, Discriminator Loss: 0.044504\n",
      "gan40000.png\n",
      "Step 41000: Generator Loss: 4.322751, Discriminator Loss: 0.308059\n",
      "Step 42000: Generator Loss: 4.695469, Discriminator Loss: 0.116527\n",
      "Step 43000: Generator Loss: 5.019681, Discriminator Loss: 0.070429\n",
      "Step 44000: Generator Loss: 4.401890, Discriminator Loss: 0.234861\n",
      "Step 45000: Generator Loss: 3.915904, Discriminator Loss: 0.243706\n",
      "Step 46000: Generator Loss: 5.589451, Discriminator Loss: 0.058797\n",
      "Step 47000: Generator Loss: 4.603510, Discriminator Loss: 0.164739\n",
      "Step 48000: Generator Loss: 3.852123, Discriminator Loss: 0.225883\n",
      "Step 49000: Generator Loss: 3.615918, Discriminator Loss: 0.366105\n",
      "Step 50000: Generator Loss: 3.899779, Discriminator Loss: 0.273610\n",
      "gan50000.png\n",
      "Step 51000: Generator Loss: 3.414676, Discriminator Loss: 0.330035\n",
      "Step 52000: Generator Loss: 3.538170, Discriminator Loss: 0.466946\n",
      "Step 53000: Generator Loss: 3.187866, Discriminator Loss: 0.493955\n",
      "Step 54000: Generator Loss: 3.190608, Discriminator Loss: 0.310563\n",
      "Step 55000: Generator Loss: 3.367553, Discriminator Loss: 0.384367\n",
      "Step 56000: Generator Loss: 3.271462, Discriminator Loss: 0.393069\n",
      "Step 57000: Generator Loss: 3.210490, Discriminator Loss: 0.500217\n",
      "Step 58000: Generator Loss: 3.378925, Discriminator Loss: 0.383457\n",
      "Step 59000: Generator Loss: 3.078242, Discriminator Loss: 0.383873\n",
      "Step 60000: Generator Loss: 3.110182, Discriminator Loss: 0.346935\n",
      "gan60000.png\n",
      "Step 61000: Generator Loss: 3.373453, Discriminator Loss: 0.518616\n",
      "Step 62000: Generator Loss: 3.135916, Discriminator Loss: 0.359233\n",
      "Step 63000: Generator Loss: 3.617075, Discriminator Loss: 0.424337\n",
      "Step 64000: Generator Loss: 3.595373, Discriminator Loss: 0.372766\n",
      "Step 65000: Generator Loss: 2.841929, Discriminator Loss: 0.324156\n",
      "Step 66000: Generator Loss: 3.379545, Discriminator Loss: 0.491967\n",
      "Step 67000: Generator Loss: 2.749012, Discriminator Loss: 0.433889\n",
      "Step 68000: Generator Loss: 2.879828, Discriminator Loss: 0.514550\n",
      "Step 69000: Generator Loss: 2.776972, Discriminator Loss: 0.514980\n",
      "Step 70000: Generator Loss: 2.922151, Discriminator Loss: 0.486705\n",
      "gan70000.png\n",
      "Step 71000: Generator Loss: 2.689571, Discriminator Loss: 0.365732\n",
      "Step 72000: Generator Loss: 3.282863, Discriminator Loss: 0.432215\n",
      "Step 73000: Generator Loss: 2.884196, Discriminator Loss: 0.360512\n",
      "Step 74000: Generator Loss: 3.059345, Discriminator Loss: 0.448837\n",
      "Step 75000: Generator Loss: 2.925928, Discriminator Loss: 0.464695\n",
      "Step 76000: Generator Loss: 3.006351, Discriminator Loss: 0.353155\n",
      "Step 77000: Generator Loss: 3.039750, Discriminator Loss: 0.544776\n",
      "Step 78000: Generator Loss: 3.093903, Discriminator Loss: 0.376360\n",
      "Step 79000: Generator Loss: 3.007651, Discriminator Loss: 0.434539\n",
      "Step 80000: Generator Loss: 2.887104, Discriminator Loss: 0.427908\n",
      "gan80000.png\n",
      "Step 81000: Generator Loss: 3.334366, Discriminator Loss: 0.416973\n",
      "Step 82000: Generator Loss: 2.622786, Discriminator Loss: 0.482032\n",
      "Step 83000: Generator Loss: 3.403825, Discriminator Loss: 0.341060\n",
      "Step 84000: Generator Loss: 3.138446, Discriminator Loss: 0.262660\n",
      "Step 85000: Generator Loss: 2.817120, Discriminator Loss: 0.388810\n",
      "Step 86000: Generator Loss: 2.819525, Discriminator Loss: 0.431321\n",
      "Step 87000: Generator Loss: 2.782107, Discriminator Loss: 0.520809\n",
      "Step 88000: Generator Loss: 2.645369, Discriminator Loss: 0.444807\n",
      "Step 89000: Generator Loss: 2.764766, Discriminator Loss: 0.394572\n",
      "Step 90000: Generator Loss: 3.024158, Discriminator Loss: 0.429489\n",
      "gan90000.png\n",
      "Step 91000: Generator Loss: 3.039988, Discriminator Loss: 0.392300\n",
      "Step 92000: Generator Loss: 3.199528, Discriminator Loss: 0.410874\n",
      "Step 93000: Generator Loss: 3.120884, Discriminator Loss: 0.422085\n",
      "Step 94000: Generator Loss: 2.989298, Discriminator Loss: 0.427282\n",
      "Step 95000: Generator Loss: 2.864139, Discriminator Loss: 0.455052\n",
      "Step 96000: Generator Loss: 3.045689, Discriminator Loss: 0.386903\n",
      "Step 97000: Generator Loss: 3.130718, Discriminator Loss: 0.565520\n",
      "Step 98000: Generator Loss: 3.598207, Discriminator Loss: 0.327262\n",
      "Step 99000: Generator Loss: 3.257097, Discriminator Loss: 0.412378\n",
      "Step 100000: Generator Loss: 3.125953, Discriminator Loss: 0.479723\n",
      "gan100000.png\n",
      "Step 101000: Generator Loss: 2.756723, Discriminator Loss: 0.387524\n",
      "Step 102000: Generator Loss: 3.172812, Discriminator Loss: 0.381841\n",
      "Step 103000: Generator Loss: 2.729324, Discriminator Loss: 0.447596\n",
      "Step 104000: Generator Loss: 2.875424, Discriminator Loss: 0.324570\n",
      "Step 105000: Generator Loss: 3.276886, Discriminator Loss: 0.349585\n",
      "Step 106000: Generator Loss: 3.169660, Discriminator Loss: 0.474269\n",
      "Step 107000: Generator Loss: 2.857732, Discriminator Loss: 0.437552\n",
      "Step 108000: Generator Loss: 2.766071, Discriminator Loss: 0.437454\n",
      "Step 109000: Generator Loss: 3.042796, Discriminator Loss: 0.378139\n",
      "Step 110000: Generator Loss: 2.544379, Discriminator Loss: 0.483024\n",
      "gan110000.png\n",
      "Step 111000: Generator Loss: 2.899277, Discriminator Loss: 0.370335\n",
      "Step 112000: Generator Loss: 3.190594, Discriminator Loss: 0.425321\n",
      "Step 113000: Generator Loss: 2.989181, Discriminator Loss: 0.405542\n",
      "Step 114000: Generator Loss: 2.534495, Discriminator Loss: 0.425429\n",
      "Step 115000: Generator Loss: 3.190461, Discriminator Loss: 0.456247\n",
      "Step 116000: Generator Loss: 3.037244, Discriminator Loss: 0.401170\n",
      "Step 117000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 118000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 119000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 120000: Generator Loss: nan, Discriminator Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gan120000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chao_gu\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\numpy\\core\\_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\chao_gu\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\matplotlib\\cm.py:256: RuntimeWarning: invalid value encountered in less\n",
      "  if norm and xx.max() > 1 or xx.min() < 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 121000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 122000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 123000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 124000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 125000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 126000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 127000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 128000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 129000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 130000: Generator Loss: nan, Discriminator Loss: nan\n",
      "gan130000.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-27c288d5546f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mdisc_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n\u001b[1;32m---> 14\u001b[1;33m                                 feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_workshop\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "        # Train\n",
    "        feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "        _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
    "                                feed_dict=feed_dict)\n",
    "        \n",
    "        if step % 1000 == 0 or step == 1:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (step, gl, dl))\n",
    "    \n",
    "        # Generate images from noise, using the generator network.\n",
    "        if step % 10000 == 0 or step == 1:\n",
    "            f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "            for i in range(10):\n",
    "                # Noise input.\n",
    "                z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "                g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "                g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "                # Reverse colours for better display\n",
    "                g = -1 * (g - 1)\n",
    "                for j in range(4):\n",
    "                    # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "                    img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "                                     newshape=(28, 28, 3))\n",
    "                    a[j][i].imshow(img)\n",
    "\n",
    "            plt.draw()\n",
    "            print('gan'+str(step)+'.png')\n",
    "            plt.savefig('gan'+str(step)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. [Optional] use TensorBoard to check the computation graph and loss.\n",
    "You might want to read about [variable sharing](https://www.tensorflow.org/versions/r1.1/programmers_guide/variable_scope) and [variable scope](https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow).\n",
    "\n",
    "This might be a bit more involved than the previous steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this was probably the hardest section so far since there's less hand holding. But if you could complete this exercise, this means that you can build reasonably sophisticated neural network models in TensorFlow! Look back and see how far you got :)\n",
    "\n",
    "Check [this](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) out if you're more interested in GANs.\n",
    "\n",
    "Thanks for completing this workshop. If you liked it, please `star` this repo, so that more and more people can learn about TensorFlow! Feedback is always welcome!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
