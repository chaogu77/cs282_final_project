{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 128 # Noise data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning param\n",
    "Batch_Size = 50\n",
    "Critic_Iters = 5 # for WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "c = 0.01 # threshold for weight cliping (-c,c)\n",
    "Iters = 200001 # number of generator iterations to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('g_h'):\n",
    "    g_W1 = tf.Variable(tf.random_normal([noise_dim,gen_hidden_dim]),name='g_W1')\n",
    "    g_b1 = tf.Variable(tf.random_normal([gen_hidden_dim]),name='g_b1')\n",
    "with tf.name_scope('g_o'):\n",
    "    g_W2 = tf.Variable(tf.random_normal([gen_hidden_dim,image_dim]),name='g_W2')\n",
    "    g_b2 = tf.Variable(tf.random_normal([image_dim]),name='g_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('d_h'):\n",
    "    d_W1 = tf.Variable(tf.random_normal([image_dim,disc_hidden_dim]),name='d_W1')\n",
    "    d_b1 = tf.Variable(tf.random_normal([disc_hidden_dim]),name='d_b1')\n",
    "with tf.name_scope('g_o'):\n",
    "    d_W2 = tf.Variable(tf.random_normal([disc_hidden_dim,1]),name='d_W2')\n",
    "    d_b2 = tf.Variable(tf.random_normal([1]),name='d_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "with tf.name_scope('Generator'):\n",
    "    def generator(noises, reuse=False):\n",
    "        with tf.variable_scope('generator') as scope:\n",
    "            if (reuse):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            # hidden layer with name \"g_hidden\"\n",
    "            hidden = tf.nn.relu(noises @ g_W1 + g_b1, name='gen_hidden')\n",
    "            # out layer with name \"g_out\"\n",
    "            out_images = tf.nn.sigmoid(hidden @ g_W2 + g_b2, name='gen_out')\n",
    "        return out_images\n",
    "\n",
    "# Discriminator\n",
    "with tf.name_scope('Discriminator'):\n",
    "    def discriminator(images, reuse=False):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if (reuse):\n",
    "                tf.get_variable_scope().reuse_variables()            \n",
    "            # hidden layer with name \"d_hidden\"\n",
    "            hidden = tf.nn.relu(images @ d_W1 + d_b1, name='disc_hidden')\n",
    "            # out layer with name \"d_out\"\n",
    "            out = tf.add(hidden @ d_W2, d_b2,name = 'disc_out')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = generator(gen_input)\n",
    "real_data = tf.placeholder(tf.float32, shape=[None, image_dim], name='real_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_real = discriminator(real_data)\n",
    "disc_fake = discriminator(fake_data, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "disc_vars = [var for var in tvars if 'd_' in var.name]\n",
    "gen_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=5e-5, \n",
    "    ).minimize(gen_cost, var_list=gen_vars)\n",
    "\n",
    "train_disc = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=5e-5, \n",
    "    ).minimize(disc_cost, var_list=disc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_D = [p.assign(tf.clip_by_value(p,-c,c)) for p in disc_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    loss = tf.summary.scalar('loss',-disc_cost)\n",
    "    \n",
    "    gen_W1 = tf.summary.scalar('g_W1',tf.reduce_mean(g_W1))\n",
    "    gen_b1 = tf.summary.scalar('g_b1',tf.reduce_mean(g_b1))\n",
    "    gen_W2 = tf.summary.scalar('g_W2',tf.reduce_mean(g_W2))\n",
    "    gen_b2 = tf.summary.scalar('g_b2',tf.reduce_mean(g_b2))\n",
    "    \n",
    "    disc_W1 = tf.summary.scalar('d_W1',tf.reduce_mean(d_W1))\n",
    "    disc_b1 = tf.summary.scalar('d_b1',tf.reduce_mean(d_b1))\n",
    "    disc_W2 = tf.summary.scalar('d_W2',tf.reduce_mean(d_W2))\n",
    "    disc_bb = tf.summary.scalar('d_b2',tf.reduce_mean(d_b2))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Generator Loss: 0.033745, Discriminator Loss: -0.017688\n",
      "wgan0.png\n",
      "Step 1000: Generator Loss: 3.531028, Discriminator Loss: -4.261106\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('WGAN_log',sess.graph, flush_secs = 30)\n",
    "    for step in range(Iters):\n",
    "\n",
    "        batch_x, _ = mnist.train.next_batch(Batch_Size)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[Batch_Size, noise_dim])\n",
    "        \n",
    "        # train discriminator\n",
    "        for i in range(Critic_Iters):\n",
    "            _,dl = sess.run([train_disc,disc_cost],\n",
    "                                   feed_dict={real_data:batch_x,gen_input:z})\n",
    "            _ = sess.run(clip_D)\n",
    "        \n",
    "        # train generator\n",
    "        _,gl=sess.run([train_gen,gen_cost],\n",
    "                      feed_dict={gen_input:z})\n",
    "        \n",
    "        # keep log\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (step, gl, dl))\n",
    "        if step % 100 == 0:\n",
    "            summary = sess.run(summary_op,feed_dict={gen_input:z, real_data:batch_x})\n",
    "            writer.add_summary(summary,global_step = step)\n",
    "            \n",
    "        # Generate images from noise, using the generator network.\n",
    "        if step % 10000 == 0:\n",
    "            f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "            for i in range(10):\n",
    "                # Noise input.\n",
    "                z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "                g = sess.run([fake_data], feed_dict={gen_input: z})\n",
    "                g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "                # Reverse colours for better display\n",
    "                g = -1 * (g - 1)\n",
    "                \n",
    "                for j in range(4):\n",
    "                    # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "                    img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "                                     newshape=(28, 28, 3))\n",
    "                    a[j][i].imshow(img)\n",
    "\n",
    "            plt.draw()\n",
    "            print('wgan'+str(step)+'.png')\n",
    "            plt.savefig('wgan'+str(step)+'.png')\n",
    "    print('Done!')\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
